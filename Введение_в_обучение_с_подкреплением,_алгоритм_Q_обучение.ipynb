{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-evc9218s67"
   },
   "source": [
    " \n",
    "В данном задании требуется разобраться с библиотекой Gym, которую не проходили в рамках учебных материалов -  требуется реализовать классический алгоритм Q-обучения на среде 'Taxi-v3' - т.е. нельзя использовать нейронные сети, нужно использовать Q-таблицу. В данной среде всего 500 уникальных состояний, так что Q-таблица будет работать лучше и обучаться намного быстрее, чем нейронная сеть. \n",
    "\n",
    "Нужную документацию можно найти по следующей ссылке: https://gym.openai.com/envs/Taxi-v3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSaO3ZG_ix8Z"
   },
   "source": [
    "Импортируем библиотеку gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXEiJ8UhcSdp",
    "outputId": "539ae07f-79cc-464a-a14e-247b1d678dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
      "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
      "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1376256/45929032 bytes (3.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3309568/45929032 bytes (7.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5873664/45929032 bytes (12.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8183808/45929032 bytes (17.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10625024/45929032 bytes (23.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13017088/45929032 bytes (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15040512/45929032 bytes (32.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17399808/45929032 bytes (37.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20127744/45929032 bytes (43.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22544384/45929032 bytes (49.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24215552/45929032 bytes (52.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25927680/45929032 bytes (56.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27607040/45929032 bytes (60.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30466048/45929032 bytes (66.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32694272/45929032 bytes (71.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35217408/45929032 bytes (76.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37789696/45929032 bytes (82.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39854080/45929032 bytes (86.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42344448/45929032 bytes (92.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44056576/45929032 bytes (95.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
      "  Done\n",
      "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
     ]
    }
   ],
   "source": [
    "import gym         #Импортируем библиотеку gym (встроенная в колаб)\n",
    "import numpy as np #Импортируем numpy\n",
    "import tensorflow as tf                    #Импортируем тензорфлоу\n",
    "import matplotlib.pyplot as plt            #Импортируем pyplot для визуализации данных\n",
    "import pickle                              #Импортируем модуль pickle для сохранения и загрузки данных\n",
    "import random                              #Импортируем модуль для генерации рандомных чисел\n",
    "from moviepy.editor import *               #Импортируем полезные функции из библиотеки для возпроизведение видео с результатом\n",
    "import cv2                                 #Импортируем библиотеку cv2 (используется для воспроизведение видео)\n",
    "from google.colab import output            #Импортируем функцию для управления вывода в колаб-ячейках\n",
    "\n",
    "#Импортируем тип данных deque из встроенного модуля питона\n",
    "from collections import deque   #deque - это список где автоматический удаляются старые значения при добовлении новых, чтобы не было переполнение памяти.\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDthFTtxEymu"
   },
   "source": [
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "Мы будем взаимодействовать со средой, которая называется `taxi-v3`. Агент управляет такси, которая должна забирать пассажиров и доставлять их в указанные точки. Агент имеет 6 действий (забирать, доставлять, двигаться влево, двигаться вправо, двигаться вверх, двигаться вниз).\n",
    "\n",
    "За неправильные действия (не взял пассажира, не туда сбросил пассажира, слишком долго вёз пассажира) назначаются отрициательные наргады. За своевременную доставку пассажира в нужное назначение даются положительные награды. По этому, цель алгоритма - обучиться правильно забирать и доставлять пассажиров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHB0ubDJ-heq",
    "outputId": "90364e43-f000-48cc-dfcd-167fb2472a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример состояния: 64\n",
      "Пространство действий: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')          #Создаем игровую среду через библиотеку gym\n",
    "state = env.reset()                #Начинаем новый эпизод, извлекаем состояние\n",
    "print(\"Пример состояния:\", state)  #Отображаем пример состояние (будет скалярное число)\n",
    "print(\"Пространство действий:\", env.action_space) #Смотрим на размер пространства действий - всего 6 дискретных действий можно совершить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41voPBBIj2tk",
    "outputId": "a21fc98e-6f58-4857-c91f-ce4ad9aeb625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 500 уникальных состояний\n",
      "Всего 6 дискретных действий\n"
     ]
    }
   ],
   "source": [
    "print(\"Всего\", env.observation_space.n, \"уникальных состояний\")\n",
    "print(\"Всего\", env.action_space.n, \"дискретных действий\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVPnRaDsGLUP"
   },
   "outputs": [],
   "source": [
    "# создадим Q-таблицу и заполним ее нулями\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMzLlmacJOin"
   },
   "outputs": [],
   "source": [
    "training_episodes = 5000 # количество эпизодов для тренировки.\n",
    "display_episodes = 10 # количество эпизодов после тренировки, для визуализации игры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XL6gAu6kJOlT"
   },
   "outputs": [],
   "source": [
    "# определим некоторые гиперпараметры\n",
    "alpha = 0.1 # Learning Rate\n",
    "gamma = 0.6 # Коэффициент учета будущих награждений\n",
    "epsilon = 0.1 # Параметр случайного хода Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntovdtNMJOsn",
    "outputId": "41535a3e-cf83-4d67-a241-1240b0c3a7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод: 0\n",
      "Эпизод: 1000\n",
      "Эпизод: 2000\n",
      "Эпизод: 3000\n",
      "Эпизод: 4000\n",
      "Обучение завершено.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучаем агента на 5000 эпизодов\n",
    "for i in range(training_episodes):\n",
    "    state = env.reset() # состояние среды\n",
    "    done = False\n",
    "    penalties, reward, = 0, 0\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Берем новое действие для этого состояния\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Аргмаксом получаем индекс  наибольшего значения из q_table\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action] # Получаем текущее значение из q-table.\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Обновляем q-value для текущего состояния\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10: # проверяем, не совершил ли агент наказуемое действие.\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "    if i % 1000 == 0: # количество отработанных алгоритмом вариантов (показывается каждые 1000 эпох)\n",
    "        print(f\"Эпизод: {i}\")\n",
    "\n",
    "print(\"Обучение завершено.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rd2Hl9AHJOvn",
    "outputId": "95afa1d1-0038-44fe-f3ca-82d269922a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "Шаг: 200\n",
      "Состояние: 488\n",
      "Действие: 0\n",
      "Награда: -1\n",
      "-----------------------------------------\n",
      "результат после 10 эпизодов:\n",
      "Среднее количество шагов в эпизодах: 105.1\n",
      "Средний штраф за эпизод: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Тестируем результат после Q-learning\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "\n",
    "# для каждого эпизода игры\n",
    "for _ in range(display_episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        clear_output(wait = True) \n",
    "        env.render()\n",
    "        print(f\"Шаг: {epochs}\")\n",
    "        print(f\"Состояние: {state}\")\n",
    "        print(f\"Действие: {action}\")\n",
    "        print(f\"Награда: {reward}\")\n",
    "        sleep(0.15) # задержка для комфортной визуализации \n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"результат после {display_episodes} эпизодов:\")\n",
    "print(f\"Среднее количество шагов в эпизодах: {total_epochs / display_episodes}\")\n",
    "print(f\"Средний штраф за эпизод: {total_penalties / display_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "238S1UGa-83Q"
   },
   "source": [
    "Алгоритм обучился плохо. Видимо, 5 000 эпох было недостаточно. Среднее количество шагов в эпизодах = 105, это очень много. Попробуем дообучить алгоритм еще на 10 000 эпохах и посмотрим, улучшит ли это способность алгоритма находить кратчайшие пути к цели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R9UjIGh-9Cw",
    "outputId": "fef47f76-f32a-42c9-df8c-d0dc25da3426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпизод: 0\n",
      "Эпизод: 1000\n",
      "Эпизод: 2000\n",
      "Эпизод: 3000\n",
      "Эпизод: 4000\n",
      "Эпизод: 5000\n",
      "Эпизод: 6000\n",
      "Эпизод: 7000\n",
      "Эпизод: 8000\n",
      "Эпизод: 9000\n",
      "Обучение завершено.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Добучим агента еще на 10000 эпизодов\n",
    "for i in range(10000):\n",
    "    state = env.reset() # состояние среды\n",
    "    done = False\n",
    "    penalties, reward, = 0, 0\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Берем новое действие для этого состояния\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Аргмаксом получаем индекс  наибольшего значения из q_table\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action] # Получаем текущее значение из q-table.\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Обновляем q-value для текущего состояния\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10: # проверяем, не совершил ли агент наказуемое действие.\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "    if i % 1000 == 0: # количество отработанных алгоритмом вариантов (показывается каждые 1000 эпох)\n",
    "        print(f\"Эпизод: {i}\")\n",
    "\n",
    "print(\"Обучение завершено.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnlCdbXc-9JB",
    "outputId": "066fde17-8800-412b-a823-cc66737c20d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Шаг: 15\n",
      "Состояние: 410\n",
      "Действие: 5\n",
      "Награда: 20\n",
      "Общая награда: 20\n",
      "-----------------------------------------\n",
      "результат после 10 эпизодов:\n",
      "Среднее количество шагов в эпизодах: 12.6\n",
      "Средний штраф за эпизод: 0.0\n",
      "Средняя награда за эпизод: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Тестируем результат после 15000 эпизодов обучения Q-learning\n",
    "\n",
    "total_epochs, total_penalties, total_reward = 0, 0, 0\n",
    "\n",
    "# для каждого эпизода игры\n",
    "for _ in range(display_episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        reward_all=0\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_all+=reward\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        clear_output(wait = True) \n",
    "        env.render()\n",
    "        print(f\"Шаг: {epochs}\")\n",
    "        print(f\"Состояние: {state}\")\n",
    "        print(f\"Действие: {action}\")\n",
    "        print(f\"Награда: {reward}\")\n",
    "        print(f\"Общая награда: {reward_all}\")\n",
    "        sleep(0.15) # задержка для комфортной визуализации \n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_reward+=reward_all\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"результат после {display_episodes} эпизодов:\")\n",
    "print(f\"Среднее количество шагов в эпизодах: {total_epochs / display_episodes}\")\n",
    "print(f\"Средний штраф за эпизод: {total_penalties / display_episodes}\")\n",
    "print(f\"Средняя награда за эпизод: {total_reward / display_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbTLWrcQAArF"
   },
   "source": [
    "Дообучение помогло. Среднее количество шагов в эпизодах удалось сократить до 12. Вероятно, дальшейшее дообучение может помочь еще сократить этот показатель (но это не точно))))\n",
    "\n",
    "В итоге, можно сделать вывод, что алгоритм очень быстро научился избегать наказания - штрафа в -10 очков, но долго обучался находить самые короткие пути к цели."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ДЗ Ultra-PRO |  | Введение в обучение с подкреплением, алгоритм Q-обучение | УИИ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
